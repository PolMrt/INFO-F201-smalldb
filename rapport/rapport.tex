\documentclass[utf8]{article}

\usepackage[utf8]{inputenc}

\usepackage[parfill]{parskip}

\usepackage{floatrow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listingsutf8}
\usepackage{multirow}

\usepackage{fullpage}

% -----------------------------------------------------


\title{INFO-F201 SmallDB : rapport}
\author{HENNEBICQ Jérémy\and MARKOWITCH Romain \and MARNETTE Pol}
\date{16 décembre 2022}

\renewcommand{\contentsname}{Table des matières}
\begin{document}
\maketitle
\tableofcontents

\newpage

% -----------------------------------------------------

\section{Introduction}

Ce projet est la prolongation de la première partie "tinydb" en prenant une nouvelle approche. Le but est le même : la réalisation d'une base de données en C++. Ici le serveur est différencié du client et communiquent ensemble via des sockets. Aussi, les requêtes des différents clients seront maitenant gérés par des threads individuels. Cette nouvelle approche soulève des questions concernant l'implémentation. Il faudra faire attention au niveau de l'accès concurrent à la base de données, trouver un moyen de passer les réponses au client, etc.

Dans ce rapport nous allons décrire notre implémentation, revenir sur certains points qui nous ont posés problèmes et comparer cette nouvelle implémentation à la précédente.

\section{Implémentation serveur client}

\subsection{Implémentation des mutex}

Pour éviter les accès concurrents à la base de données, nous avons du implémenter des mutex. Nous avons utilisé le pseudo-code fournit dans l'énoncé pour créer 4 fonctions qui sont appelés avant et après les lectures et les écritures. L'implémentations de celles-ci se trouve dans le fichier \texttt{guard.cpp}. Voici l'explication de son fonctionnement :
\begin{enumerate}
	\item Nous créons une mutex \texttt{new\_access} qui sera utilisé pour bloquer le processus de registration du lecteur ou de l'écrivain. Elle sera bloqué au début de chaque fonction \texttt{\_before} et unlock à la fin,
	\item Nous créons une mutex \texttt{write\_access} qui permet de restreindre l'accès à un unique écrivain à la fois. La fonction \texttt{write\_guard\_before} la verrouillera et la fonction \texttt{write\_guard\_after} la déverrouillera,
	\item Nous créons une mutex \texttt{reader\_registration} qui restreindra l'accès à la variable \texttt{readers\_count} pour éviter que deux threads la modifies en même temps,
	\item Ensuite, dans la fonction \texttt{read\_guard\_before} on regarde s'il n'y a pas encore de lecteur, si c'est le cas, nous bloquerons l'écriture et incrémenterons le nombre de lecteur,
	\item Finalement dans la fonction \texttt{read\_guard\_after} on décrémentera le nombre de lecteur et si celui-ci est maintenant à 0, nous déverrouillerons l'accès à l'écriture.
\end{enumerate}

Cette approche peut sembler à première approche complexe. Mais celle-ci évite les soucis de famine.

\subsection{Difficultés rencontrés}

\subsubsection{Communication des résultats au client}

Pour communiquer les résultats d'une requête envoyé par le client au serveur nous avons d'abord essayé de transmettre au client un structure \texttt{query\_result\_t}. Cette structure contins un \texttt{std::vector} des étudiants sélectionnés, la requête, le type de requête (SELECT, etc), l'état de la requête et éventuellement un message d'erreur. Nous avons donc essayé d'envoyer cette structure au client pour qu'il puisse lire les résultats de sa requête. Voici le bout de code que nous avons essayé :
\begin{lstlisting}
write(socket, query_result, sizeof(query_result_t));
\end{lstlisting}
	
	Nous n'avons pas réussi à faire fonctionner la communication avec cette approche. Nous en avons déduit que communiquer un \texttt{std::vector} via les sockets n'était pas trivial. Nous avons aussi essayé avec une liste dans le style de C (avec des pointeurs) mais le soucis restait.
	
	Nous avons donc opter pour une autre solution : envoyer ligne par ligne les résultats à imprimer chez le client. Nous avons défini un marqueur d'arrête (\texttt{**} en l'occurence) qui indiquera au client la fin des réponses a afficher dans le terminal. Cette solution fonctionne mais nous laisse perplexe au niveau de la charge réseaux, une autre implémentation nécessitant moins de \texttt{read} et \texttt{write} pourrait être optimal. Voici un bout de code de notre implémentation :

\begin{lstlisting}
// Server
for (auto student: query_result.students) {
	student_to_str(buffer_response, &student, 1024);
	write(socket, buffer_response, 1024);
}
snprintf(buffer_response, 1024, "%s", RESULT_EN_MARKER.c_str());
write(socket, buffer_response, 1024);

// Client 
while ((bytes_read = read(sock, response_buffer, 1024)) > 0
	&& strcmp(response_buffer, RESULT_EN_MARKER.c_str()) != 0) {
	std::cout << response_buffer << std::endl;
}
\end{lstlisting}

\subsubsection{Nombre d'étudiants supprimés}

Quand un client exécute la requête \texttt{delete filter=value}, le serveur supprime les éléments et renvoie au client \texttt{n student(s) deleted}. Le code fournit pour ce projet contenait déjà la logique de la fonction delete mais pas un moyen de récupérer facilement le nombre d'étudiant supprimé. Cette fonction utilise \texttt{remove\_if} qui retourne un itérateur sur le vecteur. Pour compter les éléments nous avons itéré sur les éléments supprimés pour les compter. Voici un bout de code de l'implémentation :

\begin{lstlisting}
auto new_end = remove_if(db->data.begin(), db->data.end(), predicate);

for (auto s = new_end; s != db->data.end(); ++s) {
	query_result.students.push_back(*s);
}
\end{lstlisting}

\subsubsection{Test en réseau local}

Nous avons rencontré un problème lors de nos tests sur un réseau locale. Dans un premier temps, nous avons testé sur un partage de connexion fait via un iPhone mais tous les résultats retournés par le serveur (un des deux ordinateurs en local ici) étaient étranges.

\section{Bash}

\subsection{Fonctionnement}

Nous avons décider de faire un code le plus simple possible. Pour ce faire, comme toutes les fonctions demandées utilisent le PID du processus "smalldb", nous avons créé une variable globale PID qui grep smalldb (récupère le PID du processus de smalldb). Ensuite il nous restait plus qu'à envoyé un signal SIGINT pour la fonction stop (arrête le serveur):

\begin{lstlisting}
stop(){
kill -SIGINT $pid
}
\end{lstlisting}

Un signal SIGUSR1 pour la fonction sync (sauvegarde la base de donnée sur le disque):

\begin{lstlisting}
sync(){
kill -SIGUSR1 $pid
}
\end{lstlisting}

Et enfin récupérer l'IP des clients grâce à la fonction list : 

\begin{lstlisting}
list(){
lsof -i -P -n | grep "$pid" | awk '{print $9}' | cut -d- -f2 | \
sed 's/>//g' | cut -d: -f1 | sed '1d'
}
\end{lstlisting}

Les paramètres utilisés dans la fonction lsof sont expliqué dans le fichier \texttt{smalldbctl} et le problèmes rencontrés sont quant à eux expliqués dans la sous-section ci-dessous.

\subsection{Difficultés rencontrées}

Lors de la création de la fonction list, un problème est survenu avec l'affichage de l'IP. La commande ss était introuvable sur Mac et netstat ne fonctionnait pas avec les options nécessaires pour récupérer l'IP voulu. 
Nous avons donc dû trouver une autre commande nous permettant de récupérer l'IP du client.
Après de longues heures de recherche nous avons trouvé la commande lsof. Mais un nouveau problème apparu, il fallait trier les informations que la commande lsof affichait. Nous avons trouver grâce à des forums (dont Stack Overflow) comment récupérer ce que lsof affichait, comment en extraire la partie voulue, la séparer à partir d'un certain symbole ("-" dans notre cas) et aussi comment séléctionner la bonne partie après un cut. Après ça il nous restait plus qu'à retirer l'astérisque qui s'affichait avant les IP à l'aide la commande sed/'1d' qui retire la première ligne de notre echo (en bash).


\section{Tests additionnels}

Afin de tester notre implémentation, nous avons dans un premier temps créé une "Github Action" pour compiler afin tester notre code à chaque nouveaux commit. Celui-ci se contentait de lancer \texttt{make} afin de compiler le code et lançait ensuite le script bash fournit qui contient les différents tests. Dans un second temps, nous avons déployé la partie "smalldb" (le serveur) sur une machine virtuelle distante pour tester notre code dans des conditions plus concrètes.

Nous laisserons le serveur en ligne, vous pouvez donc essayer de vous y connecter sur l'adresse IP \texttt{142.132.176.27}, comme suit :

\begin{lstlisting}
./sdbsh 142.132.176.27
\end{lstlisting}


\section{Comparaison avec la première partie}

\subsection{Pipe et sockets}

Un des changements majeur qu'apporte la seconde partie du projet est la communication via sockets entre le client et le serveur. Dans la première partie, nous utilisions les pipes, non pas pour communiquer entre le client et le serveur mais pour communiquer entre les processus gérants les requêtes et le processus principale qui communique avec l'utilisateur.

Cette nouvelle approche se rapproche plus de ce que l'on pourrait retrouver en réalité. 

\subsection{Processus et threads}

Dans cette nouvelle partie, nous exécutons les requêtes des clients de manière parallèle sur des threads différents. Dans la première partie, nous avions séparé l'exécution de chaque type de requête dans des processus différents.


\subsection{Bash}

La partie Bash du projet 2  a été plus simple a réaliser que celle de la partie 1.
Pour la partie 1, le nombre d'arguments rentrés pouvait modifier le fonctionnement des fonctions alors que dans la partie 2, le seul argument à entrer est le nom de la fonction appelée.

Dans la partie 2, il suffit de récupérer le PID de \texttt{smalldb} et de lui envoyer un signal. Tandis que pour la partie 1 nous devions trouver le bon PID parmis les différents processus en cours, pour pouvoir lui envoyer le signal voulu. Cette recherche du bon PID demandait un traitement plus conséquent avant l'envoie du signal, par rapport au simple envoie du signal de la partie 2. 

\end{document}
